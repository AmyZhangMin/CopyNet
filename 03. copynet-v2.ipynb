{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataset of number sequences\n",
    "# let's assume that we have a vocabulary size of 1000 words\n",
    "# let's assume that 0 is the EOS token, and 1 is the SOS token, and 2 is PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from models.copynet import CopyEncoder, CopyDecoder\n",
    "from models.seq2seq import Encoder01, Decoder01\n",
    "from logger import Logger\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "torch.manual_seed(1000)\n",
    "# Hyperparameters\n",
    "embed_size = 150\n",
    "hidden_size = 300\n",
    "num_layers = 1\n",
    "bin_size = 10\n",
    "num_epochs = 100\n",
    "batch_size = 20\n",
    "lr = 0.001\n",
    "vocab_size = 100\n",
    "\n",
    "step = 0 # number of steps taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "# with open('data/copynet_data_simple.txt') as f:\n",
    "with open('data/copynet_data_v2.txt') as f:\n",
    "    lines = f.readlines()\n",
    "import random\n",
    "# random.shuffle(lines)\n",
    "half = int(len(lines)/2)\n",
    "# train = lines[:half]\n",
    "# test = lines[half:]\n",
    "train = lines[200:300]\n",
    "test = lines[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = Logger('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def toData(batch):\n",
    "    # [input] batch: list of strings\n",
    "    # [output] input_out, output_out: np array([b x seq]), fixed size, eos & zero padding applied\n",
    "    # [output] in_idx, out_idx: np.array([b]), length of each line in seq \n",
    "    batch = [line.replace('\\n','') for line in batch]\n",
    "    inputs_ = []\n",
    "    outputs_ = []\n",
    "    in_len = []\n",
    "    out_len = []\n",
    "    for line in batch:\n",
    "        inputs, outputs, _ = line.split('\\t')\n",
    "        inputs_.append([int(num) for num in inputs.split(',')]+[1])\n",
    "        outputs_.append([int(num) for num in outputs.split(',')]+[1])\n",
    "        in_len.append(len(inputs_[-1]))\n",
    "        out_len.append(len(outputs_[-1]))\n",
    "    in_len = np.array(in_len)\n",
    "    out_len = np.array(out_len)\n",
    "    max_in = max(in_len)\n",
    "    max_out = max(out_len)\n",
    "    batch_size = len(batch)\n",
    "    input_out = np.zeros([batch_size,max_in],dtype=int)\n",
    "    output_out = np.zeros([batch_size,max_out],dtype=int)\n",
    "    for b in range(batch_size):\n",
    "        input_out[b][:in_len[b]] = np.array(inputs_[b])\n",
    "        output_out[b][:out_len[b]] = np.array(outputs_[b])\n",
    "    out_rev = out_len.argsort()[::-1]\n",
    "#     return input_out, output_out, in_len, out_len    \n",
    "    return input_out[out_rev], output_out[out_rev], in_len[out_rev], out_len[out_rev]\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def visualize(x):\n",
    "    plt.pcolor(x.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = len(train)\n",
    "num_batches = int(num_samples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Epoch  1\n",
      "-----------------------------\n",
      "Inputs:\n",
      "[[64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64]\n",
      " [58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58]\n",
      " [ 3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      " [ 7 70 56 17 20 85 89 74 50 46 65 89 54 67 99 66 98 14 94 54]\n",
      " [32 63 12 82 85 20  9 22 59 76 72 29 84 10 14 45  5 33 13 47]\n",
      " [35 80 19 10 28 85 17 51 45 83 25 95 13 24 83 29 57 10 64 32]\n",
      " [76 26 87 14  9 65 20 78 53 77 69 78 66 30 90 96 85  8 16 76]\n",
      " [68 10 99 52 54 32  5 97 15 41 70 23 60  5 69 16 76 63 49 19]\n",
      " [ 3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      " [65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65]\n",
      " [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]]\n",
      "Ground truth: \n",
      "[[37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37]\n",
      " [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85]\n",
      " [63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63]\n",
      " [ 7 70 56 17 20 85 89 74 50 46 65 89 54 67 99 66 98 14 94 54]\n",
      " [32 63 12 82 85 20  9 22 59 76 72 29 84 10 14 45  5 33 13 47]\n",
      " [35 80 19 10 28 85 17 51 45 83 25 95 13 24 83 29 57 10 64 32]\n",
      " [76 26 87 14  9 65 20 78 53 77 69 78 66 30 90 96 85  8 16 76]\n",
      " [68 10 99 52 54 32  5 97 15 41 70 23 60  5 69 16 76 63 49 19]\n",
      " [41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41]\n",
      " [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30]\n",
      " [79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79]\n",
      " [20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20]\n",
      " [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]]\n",
      "Predictions: \n",
      "[[37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37]\n",
      " [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85]\n",
      " [63 63 63 63 85 85 63 63 63 63 63 63 63 63 63 63 85 63 63 63]\n",
      " [63 63 63 63 63 85 63 63 63 63 63 63 63 63 63 63 63 63 63 63]\n",
      " [ 3 70 56 17 20 85 89 74 50 46 65 89 54 67 99 66 98 14 94 54]\n",
      " [32 63 12 82 85 20  9 22 59 76 72 29 84 10 14 45  5 63 13 47]\n",
      " [35 80 19 10 28 85 17 51 45 83 25 95 13 24 83 29 57 10 64 32]\n",
      " [76 26 87 14  9 65 20 78 53 77 69 78 66 30 90 96 85  8 16 76]\n",
      " [68 10 99 52 54 32  5 97 15 41 70 23 60  5 69 16 76 63 49 19]\n",
      " [30 30 30 30 30 30 30 30 30 41 30 30 30 30 30 30 30 30 30 30]\n",
      " [79 79 79 79 79 79 79 79 79 79 79 79 79 30 79 79 79 79 79 79]\n",
      " [20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20]\n",
      " [ 1  1  1  1 20 20 20  1  1  1  1  1  1  1  1  1  1  1  1  1]]\n",
      "Loss:  4.5644731521606445\n",
      "==================================================\n",
      "Epoch  2\n",
      "Loss:  4.39904260635376\n",
      "==================================================\n",
      "Epoch  3\n",
      "Loss:  4.208532333374023\n",
      "==================================================\n",
      "Epoch  4\n",
      "Loss:  4.306004524230957\n",
      "==================================================\n",
      "Epoch  5\n",
      "Loss:  4.115550518035889\n",
      "==================================================\n",
      "Epoch  6\n",
      "Loss:  4.318385124206543\n",
      "==================================================\n",
      "Epoch  7\n",
      "Loss:  4.101155757904053\n",
      "==================================================\n",
      "Epoch  8\n",
      "Loss:  4.303430080413818\n",
      "==================================================\n",
      "Epoch  9\n",
      "Loss:  4.088895320892334\n",
      "==================================================\n",
      "Epoch  10\n",
      "Loss:  4.306859970092773\n",
      "==================================================\n",
      "Epoch  11\n",
      "Loss:  4.107330799102783\n",
      "==================================================\n",
      "Epoch  12\n",
      "Loss:  4.3144683837890625\n",
      "==================================================\n",
      "Epoch  13\n",
      "Loss:  4.099520206451416\n",
      "==================================================\n",
      "Epoch  14\n",
      "Loss:  4.310630798339844\n",
      "==================================================\n",
      "Epoch  15\n",
      "Loss:  4.118544101715088\n",
      "==================================================\n",
      "Epoch  16\n",
      "Loss:  4.310800552368164\n",
      "==================================================\n",
      "Epoch  17\n",
      "Loss:  4.110879421234131\n",
      "==================================================\n",
      "Epoch  18\n",
      "Loss:  4.29925012588501\n",
      "==================================================\n",
      "Epoch  19\n",
      "Loss:  4.114614486694336\n",
      "==================================================\n",
      "Epoch  20\n",
      "Loss:  4.318404674530029\n",
      "==================================================\n",
      "Epoch  21\n",
      "-----------------------------\n",
      "Inputs:\n",
      "[[64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64]\n",
      " [58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58]\n",
      " [ 3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      " [71 89 95 35 72  5 30 74 70 66 99 52 43 51 69 87 64 40 11 62]\n",
      " [10 29 97 96 75 79 13 22 63 45 13 84 19 86 38 94 89  5 44 77]\n",
      " [70 95 57 52 39 24 22 51 80 29 29 77 96  4 28 88 54 53 98 13]\n",
      " [ 9 78 31 17 26 35 35 78 26 96 89 36 55 72  5 55 27 41 99 17]\n",
      " [55 23  9 10 44 77 87 97 10 16 51 47 16 73 82 88 74 55 62 29]\n",
      " [ 3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      " [65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65]\n",
      " [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]]\n",
      "Ground truth: \n",
      "[[37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37]\n",
      " [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85]\n",
      " [63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63]\n",
      " [71 89 95 35 72  5 30 74 70 66 99 52 43 51 69 87 64 40 11 62]\n",
      " [10 29 97 96 75 79 13 22 63 45 13 84 19 86 38 94 89  5 44 77]\n",
      " [70 95 57 52 39 24 22 51 80 29 29 77 96  4 28 88 54 53 98 13]\n",
      " [ 9 78 31 17 26 35 35 78 26 96 89 36 55 72  5 55 27 41 99 17]\n",
      " [55 23  9 10 44 77 87 97 10 16 51 47 16 73 82 88 74 55 62 29]\n",
      " [41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41]\n",
      " [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30]\n",
      " [79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79]\n",
      " [20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20]\n",
      " [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]]\n",
      "Predictions: \n",
      "[[37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37]\n",
      " [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85]\n",
      " [63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63]\n",
      " [30 30 30 30 30 30 30 30 63 30 30 30 30 30 30 30 30 30 30 30]\n",
      " [71 89 95 35 72  5 30 74 70 66 99 52 43 51 69 87 64 40 11 62]\n",
      " [10 29 97 96 75 79 13 22 63 45 13 84 19 86 38 94 89  5 44 77]\n",
      " [70 95 57 52 39 24 22 51 80 29 29 77 96  4 28 88 54 53 98 13]\n",
      " [ 9 78 31 17 26 35 35 78 26 96 89 36 55 72  5 55 27 41 99 17]\n",
      " [55 23  9 10 44 77 87 97 10 16 51 47 16 73 82 88 74 55 62 29]\n",
      " [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 41 30 30]\n",
      " [79 79 79 79 79 79 30 79 79 79 79 79 79 79 79 79 79 79 79 79]\n",
      " [20 20 20 20 20 79 20 20 20 20 20 20 20 20 20 20 20 20 20 20]\n",
      " [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]]\n",
      "Loss:  4.091612339019775\n",
      "==================================================\n",
      "Epoch  22\n",
      "Loss:  4.322202205657959\n",
      "==================================================\n",
      "Epoch  23\n",
      "Loss:  4.099235534667969\n",
      "==================================================\n",
      "Epoch  24\n",
      "Loss:  4.314488887786865\n",
      "==================================================\n",
      "Epoch  25\n",
      "Loss:  4.099260330200195\n",
      "==================================================\n",
      "Epoch  26\n",
      "Loss:  4.3030104637146\n",
      "==================================================\n",
      "Epoch  27\n",
      "Loss:  4.103112697601318\n",
      "==================================================\n",
      "Epoch  28\n",
      "Loss:  4.318397521972656\n",
      "==================================================\n",
      "Epoch  29\n",
      "Loss:  4.106943607330322\n",
      "==================================================\n",
      "Epoch  30\n",
      "Loss:  4.310698509216309\n",
      "==================================================\n",
      "Epoch  31\n",
      "Loss:  4.1069488525390625\n",
      "==================================================\n",
      "Epoch  32\n",
      "Loss:  4.318390369415283\n",
      "==================================================\n",
      "Epoch  33\n",
      "Loss:  4.095341205596924\n",
      "==================================================\n",
      "Epoch  34\n",
      "Loss:  4.329897403717041\n",
      "==================================================\n",
      "Epoch  35\n",
      "Loss:  4.118449687957764\n",
      "==================================================\n",
      "Epoch  36\n",
      "Loss:  4.306845664978027\n",
      "==================================================\n",
      "Epoch  37\n",
      "Loss:  4.106904983520508\n",
      "==================================================\n",
      "Epoch  38\n",
      "Loss:  4.314486026763916\n",
      "==================================================\n",
      "Epoch  39\n",
      "Loss:  4.103046417236328\n",
      "==================================================\n",
      "Epoch  40\n",
      "Loss:  4.306819438934326\n",
      "==================================================\n",
      "Epoch  41\n",
      "-----------------------------\n",
      "Inputs:\n",
      "[[64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64]\n",
      " [58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58]\n",
      " [ 3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      " [23 56 72 80 30 99 86 66 57 95 79 52 66 51 11 14 20 70 60 14]\n",
      " [75 12 75 20 40 14 15 45 51 15 47 84  7 86 71 33 17 57 96 60]\n",
      " [44 19 39 14 66 83 42 29 99 73 96 77 35  4 32 10 31 66 93 79]\n",
      " [29 87 26 16 87 90 74 96 97 10 23 36 52 72 88  8 75 19  7 40]\n",
      " [19 99 44 82 68 69 53 16 11 19 99 47 35 73 30 63 84 61 13 90]\n",
      " [ 3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      " [65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65]\n",
      " [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]]\n",
      "Ground truth: \n",
      "[[37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37]\n",
      " [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85]\n",
      " [63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63]\n",
      " [23 56 72 80 30 99 86 66 57 95 79 52 66 51 11 14 20 70 60 14]\n",
      " [75 12 75 20 40 14 15 45 51 15 47 84  7 86 71 33 17 57 96 60]\n",
      " [44 19 39 14 66 83 42 29 99 73 96 77 35  4 32 10 31 66 93 79]\n",
      " [29 87 26 16 87 90 74 96 97 10 23 36 52 72 88  8 75 19  7 40]\n",
      " [19 99 44 82 68 69 53 16 11 19 99 47 35 73 30 63 84 61 13 90]\n",
      " [41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41]\n",
      " [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30]\n",
      " [79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79]\n",
      " [20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20]\n",
      " [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]]\n",
      "Predictions: \n",
      "[[37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37]\n",
      " [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85]\n",
      " [63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63]\n",
      " [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 63 30 30 30 30]\n",
      " [23 56 72 80 30 99 86 66 57 95 79 52 66 51 11 14 20 70 60 14]\n",
      " [75 12 75 20 40 14 15 45 51 15 47 84  7 86 71 33 17 57 96 60]\n",
      " [44 19 39 14 66 83 42 29 99 73 96 77 35  4 32 10 31 66 93 79]\n",
      " [29 87 26 16 87 90 74 96 97 10 23 36 52 72 88  8 75 19  7 40]\n",
      " [19 99 44 82 68 69 53 16 11 19 99 47 35 73 30 63 84 61 13 90]\n",
      " [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30]\n",
      " [79 79 79 79 30 79 79 79 79 79 79 79 79 79 30 79 79 79 79 79]\n",
      " [20 20 20 20 20 20 20 20 20 20 79 20 20 20 20 20 20 20 20 79]\n",
      " [ 1  1  1 20  1  1  1  1  1  1  1  1  1  1  1  1 20  1  1  1]]\n",
      "Loss:  4.103044033050537\n",
      "==================================================\n",
      "Epoch  42\n"
     ]
    }
   ],
   "source": [
    "################ copynet model #####################\n",
    "encoder = CopyEncoder(vocab_size, embed_size, hidden_size)\n",
    "decoder = CopyDecoder(vocab_size, embed_size, hidden_size)\n",
    "opt_e = optim.Adam(params=encoder.parameters(), lr=lr)\n",
    "opt_d = optim.Adam(params=decoder.parameters(), lr=lr)\n",
    "# encoder = torch.load(f='models/encoder01.pth')\n",
    "# decoder = torch.load(f='models/decoder01.pth')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "for epoch in range(num_epochs):\n",
    "#     if epoch % 20 == 19:\n",
    "#         lr=lr/10\n",
    "    print(\"==================================================\")\n",
    "    print(\"Epoch \",epoch+1)\n",
    "#     opt_e = optim.SGD(params=encoder.parameters(), lr=lr)\n",
    "#     opt_d = optim.SGD(params=decoder.parameters(), lr=lr)\n",
    "    \n",
    "    # shuffle data\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # initialize gradient buffers\n",
    "        opt_e.zero_grad()\n",
    "        opt_d.zero_grad()\n",
    "\n",
    "        # obtain batch outputs\n",
    "        batch = train[i*batch_size:(i+1)*batch_size]\n",
    "        input_out, output_out, in_len, out_len = toData(batch)\n",
    "        \n",
    "        # mask input to remove padding\n",
    "        input_mask = np.array(input_out>0, dtype=int)\n",
    "        \n",
    "        # input and output in Variable form\n",
    "        x = torch.LongTensor(input_out)\n",
    "        y = torch.LongTensor(output_out)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        x = Variable(x)\n",
    "        y = Variable(y)\n",
    "#         print(\"input\",x)\n",
    "        encoded, _ = encoder(x)\n",
    "        \n",
    "        # get initial input of decoder\n",
    "        decoder_in = torch.LongTensor(np.ones(x.size(0),dtype=int))*2\n",
    "        s = None\n",
    "        w = None\n",
    "        if torch.cuda.is_available():\n",
    "            decoder_in = decoder_in.cuda()\n",
    "        decoder_in = Variable(decoder_in)\n",
    "        \n",
    "        # out_list to store outputs\n",
    "        out_list=[]\n",
    "#         for j in range(3): # for all sequences\n",
    "        for j in range(y.size(1)): # for all sequences\n",
    "            \"\"\"\n",
    "            decoder_in (Variable): [b]\n",
    "            encoded (Variable): [b x seq x hid]\n",
    "            input_out (np.array): [b x seq]\n",
    "            s (Variable): [b x hid]\n",
    "            \"\"\"\n",
    "            # calculate for 1st state\n",
    "            if j==0:\n",
    "                out, s, w = decoder(input_idx=decoder_in, encoded=encoded,\n",
    "                                encoded_idx=input_out, prev_state=s, \n",
    "                                weighted=w, order=j)\n",
    "            else:\n",
    "                tmp_out, s, w = decoder(input_idx=decoder_in, encoded=encoded,\n",
    "                                encoded_idx=input_out, prev_state=s, \n",
    "                                weighted=w, order=j)\n",
    "                out = torch.cat([out,tmp_out],dim=1)\n",
    "            # select next input\n",
    "            if epoch % 2 ==1:\n",
    "                decoder_in = out[:,-1].max(1)[1].squeeze() # train with sequence outputs\n",
    "            else:\n",
    "                decoder_in = y[:,j] # train with ground truth\n",
    "            out_list.append(out[:,-1].max(1)[1].squeeze().cpu().data.numpy())\n",
    "        # get loss\n",
    "        target = pack_padded_sequence(y,out_len.tolist(), batch_first=True)[0]\n",
    "        pad_out = pack_padded_sequence(out,out_len.tolist(), batch_first=True)[0]\n",
    "        loss = criterion(pad_out, target)\n",
    "        loss.backward()\n",
    "        opt_e.step()\n",
    "        opt_d.step()\n",
    "        step += 1\n",
    "        info = {\n",
    "            'loss': loss.data[0]\n",
    "        }\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag,value,step)\n",
    "        \n",
    "        for tag, value in encoder.named_parameters():\n",
    "            tag = 'encoder/'+tag\n",
    "            logger.histo_summary(tag, to_np(value), step)\n",
    "            logger.histo_summary(tag+'/grad', to_np(value.grad), step)\n",
    "\n",
    "        for tag, value in decoder.named_parameters():\n",
    "            tag = 'decoder/'+tag\n",
    "            logger.histo_summary(tag, to_np(value), step)\n",
    "            logger.histo_summary(tag+'/grad', to_np(value.grad), step)\n",
    "    \n",
    "    # print loss\n",
    "    if epoch % 20==0:\n",
    "        print(\"-----------------------------\")\n",
    "        print(\"Inputs:\")\n",
    "        print(x.cpu().data.numpy().transpose())\n",
    "        print(\"Ground truth: \")\n",
    "        print(y.cpu().data.numpy().transpose())\n",
    "        print(\"Predictions: \")\n",
    "        print(np.array(out_list))\n",
    "    print(\"Loss: \", loss.data[0])\n",
    "#     print(\"-----------------------------\")\n",
    "#     print(\"Inputs:\")\n",
    "#     print(x.cpu().data.numpy().transpose())\n",
    "#     print(\"Ground truth: \")\n",
    "#     print(y.cpu().data.numpy().transpose())\n",
    "#     print(\"Predictions: \")\n",
    "#     print(np.array(out_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################ test copynet model #####################\n",
    "# encoder = torch.load(f='models/encoder01.pth')\n",
    "# decoder = torch.load(f='models/decoder01.pth')\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "for epoch in 1:\n",
    "    if epoch % 20 == 19:\n",
    "        lr=lr/10\n",
    "    print(\"==================================================\")\n",
    "    print(\"Epoch \",epoch+1)\n",
    "    opt_e = optim.Adam(params=encoder.parameters(), lr=lr)\n",
    "    opt_d = optim.Adam(params=decoder.parameters(), lr=lr)\n",
    "#     opt_e = optim.SGD(params=encoder.parameters(), lr=lr)\n",
    "#     opt_d = optim.SGD(params=decoder.parameters(), lr=lr)\n",
    "    \n",
    "    # shuffle data\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # initialize gradient buffers\n",
    "        opt_e.zero_grad()\n",
    "        opt_d.zero_grad()\n",
    "\n",
    "        # obtain batch outputs\n",
    "        batch = train[i*batch_size:(i+1)*batch_size]\n",
    "        input_out, output_out, in_len, out_len = toData(batch)\n",
    "        \n",
    "        # mask input to remove padding\n",
    "        input_mask = np.array(input_out>0, dtype=int)\n",
    "        \n",
    "        # input and output in Variable form\n",
    "        x = torch.LongTensor(input_out)\n",
    "        y = torch.LongTensor(output_out)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        x = Variable(x)\n",
    "        y = Variable(y)\n",
    "#         print(\"input\",x)\n",
    "        encoded, _ = encoder(x)\n",
    "        \n",
    "        # get initial input of decoder\n",
    "        decoder_in = torch.LongTensor(np.ones(x.size(0),dtype=int))*2\n",
    "        s = None\n",
    "        w = None\n",
    "        if torch.cuda.is_available():\n",
    "            decoder_in = decoder_in.cuda()\n",
    "        decoder_in = Variable(decoder_in)\n",
    "        \n",
    "        # out_list to store outputs\n",
    "        out_list=[]\n",
    "#         for j in range(3): # for all sequences\n",
    "        for j in range(y.size(1)): # for all sequences\n",
    "            \"\"\"\n",
    "            decoder_in (Variable): [b]\n",
    "            encoded (Variable): [b x seq x hid]\n",
    "            input_out (np.array): [b x seq]\n",
    "            s (Variable): [b x hid]\n",
    "            \"\"\"\n",
    "            # calculate for 1st state\n",
    "            if j==0:\n",
    "                out, s, w = decoder(input_idx=decoder_in, encoded=encoded,\n",
    "                                encoded_idx=input_out, prev_state=s, \n",
    "                                weighted=w, order=j)\n",
    "            else:\n",
    "                tmp_out, s, w = decoder(input_idx=decoder_in, encoded=encoded,\n",
    "                                encoded_idx=input_out, prev_state=s, \n",
    "                                weighted=w, order=j)\n",
    "                out = torch.cat([out,tmp_out],dim=1)\n",
    "            # select next input\n",
    "            if epoch % 2 ==1:\n",
    "                decoder_in = out[:,-1].max(1)[1].squeeze() # train with sequence outputs\n",
    "            else:\n",
    "                decoder_in = y[:,j] # train with ground truth\n",
    "            out_list.append(out[:,-1].max(1)[1].squeeze().cpu().data.numpy())\n",
    "        # get loss\n",
    "        target = pack_padded_sequence(y,out_len.tolist(), batch_first=True)[0]\n",
    "        pad_out = pack_padded_sequence(out,out_len.tolist(), batch_first=True)[0]\n",
    "        loss = criterion(pad_out, target)\n",
    "        loss.backward()\n",
    "        opt_e.step()\n",
    "        opt_d.step()\n",
    "        step += 1\n",
    "        info = {\n",
    "            'loss': loss.data[0]\n",
    "        }\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag,value,step)\n",
    "        \n",
    "        for tag, value in encoder.named_parameters():\n",
    "            tag = 'encoder/'+tag\n",
    "            logger.histo_summary(tag, to_np(value), step)\n",
    "            logger.histo_summary(tag+'/grad', to_np(value.grad), step)\n",
    "\n",
    "        for tag, value in decoder.named_parameters():\n",
    "            tag = 'decoder/'+tag\n",
    "            logger.histo_summary(tag, to_np(value), step)\n",
    "            logger.histo_summary(tag+'/grad', to_np(value.grad), step)\n",
    "    \n",
    "    # print loss\n",
    "    if epoch % 20==0:\n",
    "        print(\"-----------------------------\")\n",
    "        print(\"Inputs:\")\n",
    "        print(x.cpu().data.numpy().transpose())\n",
    "        print(\"Ground truth: \")\n",
    "        print(y.cpu().data.numpy().transpose())\n",
    "        print(\"Predictions: \")\n",
    "        print(np.array(out_list))\n",
    "    print(\"Loss: \", loss.data[0])\n",
    "#     print(\"-----------------------------\")\n",
    "#     print(\"Inputs:\")\n",
    "#     print(x.cpu().data.numpy().transpose())\n",
    "#     print(\"Ground truth: \")\n",
    "#     print(y.cpu().data.numpy().transpose())\n",
    "#     print(\"Predictions: \")\n",
    "#     print(np.array(out_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(torch.stack(decoder.prob_c_to_g,dim=1)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.stack(decoder.prob_c_to_g,dim=1)[:,0].max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(torch.stack(decoder.probs,dim=1)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.stack(decoder.probs,dim=1)[:,0,100:].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(torch.stack(decoder.prob_c_to_g,dim=1)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(out[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(torch.stack(decoder.attn,dim=1)[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out.max(2)[1].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.stack(decoder.W,1).squeeze()[:,1].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
